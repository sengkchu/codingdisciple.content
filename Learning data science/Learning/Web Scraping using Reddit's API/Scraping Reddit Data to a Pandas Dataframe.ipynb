{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will try to scrape data from reddit using their API. The objective is to load reddit data into a pandas dataframe. In order to achieve this, first we'll import the following libraries.\n",
    "\n",
    "The documentation for this API can be found [here](https://github.com/reddit/reddit/wiki/API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request as ur\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the raw json data of any subreddit by adding '.json' to the URL. Using the urllib.request library, we can extract that data and read it in python.\n",
    "\n",
    "From the documentation, we need to fill out the header using the suggested format.\n",
    "\n",
    "Example: User-Agent: android:com.example.myredditapp:v1.2.3 (by /u/kemitche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Header to be submitted to reddit.\n",
    "hdr = {'User-Agent': 'codingdisciple:playingwithredditAPI:v1.0 (by /u/ivis_reine)'}\n",
    "\n",
    "#Link to the subreddit of interest.\n",
    "url = \"https://www.reddit.com/r/datascience/.json?sort=top&t=all\"\n",
    "\n",
    "#Makes a request object and receive a response.\n",
    "req = ur.Request(url, headers=hdr)\n",
    "response = ur.urlopen(req).read()\n",
    "\n",
    "#Loads the json data into python.\n",
    "json_data = json.loads(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took a snapshot of the data structure below. It looks like the data is just a bunch of lists and dictionaries. We want reach the part of the dictionary until we see a list. Each item on this list will be a post made on this subreddit.\n",
    "\n",
    "![reddit_data_structure](images/scraping-reddit-data-pandas-dataframe/datastructure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The actual data starts.\n",
    "data = json_data['data']['children']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each request can only get us 100 posts, we can write a for loop to send 10 requests at 2 second intervals and add the data to the list of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 posts loaded\n",
      "225 posts loaded\n",
      "325 posts loaded\n",
      "425 posts loaded\n",
      "525 posts loaded\n",
      "625 posts loaded\n",
      "725 posts loaded\n",
      "825 posts loaded\n",
      "898 posts loaded\n",
      "898 posts loaded\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    #reddit only accepts one request every 2 seconds, adds a delay between each loop\n",
    "    time.sleep(2)\n",
    "    last = data[-1]['data']['name']\n",
    "    url = 'https://www.reddit.com/r/datascience/.json?sort=top&t=all&limit=100&after=%s' % last\n",
    "    req = ur.Request(url, headers=hdr)\n",
    "    text_data = ur.urlopen(req).read()\n",
    "    datatemp = json.loads(text_data.decode('utf-8'))\n",
    "    data += datatemp['data']['children']\n",
    "    print(str(len(data))+\" posts loaded\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've assigned all the posts to a list with the variable named 'data'. In order to begin constructing our pandas dataframe, we need a list of column names. Each post consists of a dictionary, we can simply loop through this dictionary and extract the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contest_mode', 'num_reports', 'subreddit_type', 'report_reasons', 'spoiler', 'quarantine', 'author', 'media_embed', 'banned_at_utc', 'is_self', 'selftext_html', 'stickied', 'banned_by', 'author_flair_css_class', 'approved_by', 'mod_reason_title', 'author_flair_text', 'selftext', 'suggested_sort', 'hide_score', 'brand_safe', 'subreddit', 'hidden', 'link_flair_text', 'mod_note', 'num_comments', 'name', 'pinned', 'user_reports', 'thumbnail', 'subreddit_name_prefixed', 'score', 'can_mod_post', 'can_gild', 'mod_reason_by', 'edited', 'downs', 'domain', 'is_crosspostable', 'clicked', 'approved_at_utc', 'title', 'removal_reason', 'likes', 'is_reddit_media_domain', 'locked', 'view_count', 'id', 'over_18', 'distinguished', 'visited', 'media', 'gilded', 'link_flair_css_class', 'secure_media', 'is_video', 'mod_reports', 'permalink', 'secure_media_embed', 'parent_whitelist_status', 'saved', 'num_crossposts', 'url', 'archived', 'created', 'created_utc', 'subreddit_id', 'whitelist_status', 'ups']\n"
     ]
    }
   ],
   "source": [
    "#Create a list of column name strings to be used to create our pandas dataframe\n",
    "data_names = [value for value in data[0]['data']]\n",
    "print(data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a dataframe using the pd.DataFrame() function, we will need a list of dictionaries. \n",
    "\n",
    "We can loop through each element in 'data', using each column name as a key to the dictionary, then accessing the corresponding value with that key. If we come across a post that has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a list of dictionaries to be loaded into a pandas dataframe\n",
    "df_loadinglist = []\n",
    "for i in range(0, len(data)):\n",
    "    dictionary = {}\n",
    "    for names in data_names:\n",
    "        try:\n",
    "            dictionary[str(names)] = data[i]['data'][str(names)]\n",
    "        except:\n",
    "            dictionary[str(names)] = 'None'\n",
    "    df_loadinglist.append(dictionary)\n",
    "df=pd.DataFrame(df_loadinglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 69)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>brand_safe</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>amoun1365</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Is there any good statistics specialization (M...</td>\n",
       "      <td>10</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>sananth12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Carla – Open source simulator for autonomous d...</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.carla.org/</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>blacksite_</td>\n",
       "      <td>testflair</td>\n",
       "      <td>BS (Economics) | Data Scientist | IT Operations</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>What statistical methods/tools do you use most?</td>\n",
       "      <td>44</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>thatneedle</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Free, Fast Entity Extraction - requesting feed...</td>\n",
       "      <td>0</td>\n",
       "      <td>http://entity.thatneedle.com</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>basketballwonk</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>I'm at a crossroads and looking for advice (st...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    approved_at_utc approved_by  archived          author  \\\n",
       "893            None        None     False       amoun1365   \n",
       "894            None        None     False       sananth12   \n",
       "895            None        None     False      blacksite_   \n",
       "896            None        None     False      thatneedle   \n",
       "897            None        None     False  basketballwonk   \n",
       "\n",
       "    author_flair_css_class                                author_flair_text  \\\n",
       "893                   None                                             None   \n",
       "894                   None                                             None   \n",
       "895              testflair  BS (Economics) | Data Scientist | IT Operations   \n",
       "896                   None                                             None   \n",
       "897                   None                                             None   \n",
       "\n",
       "    banned_at_utc banned_by  brand_safe  can_gild        ...         \\\n",
       "893          None      None        True     False        ...          \n",
       "894          None      None        True     False        ...          \n",
       "895          None      None        True     False        ...          \n",
       "896          None      None        True     False        ...          \n",
       "897          None      None        True     False        ...          \n",
       "\n",
       "     subreddit_type  suggested_sort  thumbnail  \\\n",
       "893          public            None              \n",
       "894          public            None              \n",
       "895          public            None              \n",
       "896          public            None              \n",
       "897          public            None              \n",
       "\n",
       "                                                 title  ups  \\\n",
       "893  Is there any good statistics specialization (M...   10   \n",
       "894  Carla – Open source simulator for autonomous d...    4   \n",
       "895    What statistical methods/tools do you use most?   44   \n",
       "896  Free, Fast Entity Extraction - requesting feed...    0   \n",
       "897  I'm at a crossroads and looking for advice (st...    2   \n",
       "\n",
       "                                                   url user_reports  \\\n",
       "893  https://www.reddit.com/r/datascience/comments/...           []   \n",
       "894                              http://www.carla.org/           []   \n",
       "895  https://www.reddit.com/r/datascience/comments/...           []   \n",
       "896                       http://entity.thatneedle.com           []   \n",
       "897  https://www.reddit.com/r/datascience/comments/...           []   \n",
       "\n",
       "     view_count visited  whitelist_status  \n",
       "893        None   False           all_ads  \n",
       "894        None   False           all_ads  \n",
       "895        None   False           all_ads  \n",
       "896        None   False           all_ads  \n",
       "897        None   False           all_ads  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have a pandas dataframe, we can do simple analysis on the reddit posts. For example, we can write a function to find the most common words used in the last 925 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to     2820\n",
       "the    2670\n",
       "i      2575\n",
       "a      2554\n",
       "and    2189\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counts each word and return a pandas series\n",
    "\n",
    "def word_count(df, column):\n",
    "    dic={}\n",
    "    for idx, row in df.iterrows():\n",
    "        split = row[column].lower().split(\" \")\n",
    "        for word in split:\n",
    "            if word in dic:\n",
    "                dic[word] += 1\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "    dictionary = pd.Series(dic)\n",
    "    dictionary = dictionary.sort_values(ascending=False)\n",
    "    return dictionary\n",
    "\n",
    "top_counts = word_count(df, \"selftext\")\n",
    "top_counts[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results are not too surprising, common english words showed up the most. That is it for now! We've achieved our goal of turning json data into a pandas dataframe.\n",
    "\n",
    "---\n",
    "\n",
    "#### Learning Summary\n",
    "\n",
    "Concepts explored: lists, dictionaries, API, data structures, JSON\n",
    "\n",
    "The files for this project can be found in my [GitHub repository](https://github.com/sengkchu/codingdisciple.content/tree/master/Learning%20data%20science/Learning/Web%20Scraping%20using%20Reddit's%20API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
